> Provide a detailed checklist (at least eight bullet points) of best practices for testing command-line
  automation harnesses that interact with tmux sessions.

✦ Here is a detailed checklist of best practices for testing command-line automation harnesses that
  interact with tmux sessions:

   * Use Isolated and Ephemeral Sessions: Each test case should create a tmux session with a unique,
     randomly generated name. This prevents collisions between concurrent tests or interference with a
     user's active sessions. Ensure that every test, whether it passes or fails, destroys its session
     using a reliable teardown mechanism (like pytest fixtures or a try...finally block) to prevent
     orphaned processes.

   * Implement Stateful Synchronization, Not Fixed Delays: Never use fixed-duration waits (e.g.,
     time.sleep(5)). Instead, build functions that repeatedly capture the tmux pane and check for an
     expected state—such as the appearance of a shell prompt, a "Done" message, or a specific UI element.
     This makes tests faster and far more reliable by waiting only as long as necessary.

   * Verify State via Pane Content Capture: The primary method for assertion should be capturing the pane
     content (tmux capture-pane -p) and validating its text. Treat the pane content as the "source of
     truth" for the state of the application inside tmux. Use string matching, regular expressions, or
     more advanced parsing to confirm that commands executed correctly and the UI is in the expected
     state.

   * Abstract All Tmux Interactions: Do not litter your test logic with direct subprocess calls to the
     tmux binary. Create a dedicated "driver" or "controller" class that provides a high-level API (e.g.,
     session.send_keys("ls -l"), session.wait_for_prompt(), session.get_pane_text()). This makes tests
     cleaner, more readable, and allows you to swap out the backend or mock the driver for unit tests.

   * Test for Failure and Edge Cases: A robust harness must handle things going wrong. Create specific
     tests for scenarios like: a command inside tmux returning a non-zero exit code, an application
     crashing, or output that doesn't match expectations. The harness should be able to detect, report,
     and recover from these failures gracefully.

   * Design for Headless Execution: Ensure your test suite can run in environments without a physical
     display or user-attached terminal, such as a CI/CD pipeline. Use tmux start-server to initialize the
     tmux server in the background and manage all sessions programmatically.

   * Use Flexible Marker and Prompt Detection: Avoid hardcoding exact shell prompts (e.g.,
     user@hostname:~$). These can change between environments or shells. Instead, use regular expressions
     that flexibly match the pattern of a prompt, or better yet, have your harness write a unique,
     easy-to-find marker to the session after each command to signal completion.

   * Manage Pane/Window State Explicitly: Do not assume which pane or window is active. If your automation
      involves multiple panes or windows, your test harness must explicitly target each one for sending
     keys or capturing text using its unique ID (e.g., tmux send-keys -t session:0.1). Tests should
     validate that the window/pane layout is as expected before proceeding.

                                                                          YOLO mode (ctrl + y to toggle)
╭──────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ *   Type your message or @path/to/file                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯
 /mnt//Orechestrator2             no sandbox (see /docs)              gemini-2.5-pro (99% context left)

